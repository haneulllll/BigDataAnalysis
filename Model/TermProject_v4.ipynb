{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9698ac93-6ed2-45dd-b7d3-264dd265a148",
   "metadata": {},
   "source": [
    "## csv ë°ì´í„° ê¸°ë°˜ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "eaf6e202-a51b-48f8-905f-7a287c87949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class AISPreprocessor:\n",
    "    # data_dir: ê²½ë¡œ ë°ì´í„° íŒŒì¼(csv)ì´ ì¡´ì¬í•˜ëŠ” í´ë” ì´ë¦„ \n",
    "    # input_seq_len: ì°¸ì¡°í•  ì´ì „ ê³¼ê±° ì •ë³´(default: 10ë¶„)\n",
    "    # output_seq_len: ì˜ˆì¸¡í•  ë¯¸ë˜ ì •ë³´(default: 1ë¶„)\n",
    "    def __init__(self, data_dir, input_seq_len=10, output_seq_len=1):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_seq_len = input_seq_len\n",
    "        self.output_seq_len = output_seq_len\n",
    "\n",
    "        # ìˆ˜ë™ ì„¤ì •ëœ ë²”ìœ„ë¡œ MinMaxScaler ì´ˆê¸°í™”\n",
    "        # ì •ê·œí™” ë²”ìœ„ ì„¤ì •\n",
    "        lat_range = (33.0, 38.0)\n",
    "        lon_range = (124.0, 132.0)\n",
    "        sog_range = (0.0, 100.0)\n",
    "        cog_range = (0.0, 360.0)\n",
    "        heading_range = (0.0, 360.0)\n",
    "        \n",
    "        # MinMaxScaler ìˆ˜ë™ ì„¤ì •\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.scaler.min_ = np.array([\n",
    "            -lat_range[0] / (lat_range[1] - lat_range[0]),\n",
    "            -lon_range[0] / (lon_range[1] - lon_range[0]),\n",
    "            -sog_range[0] / (sog_range[1] - sog_range[0]),\n",
    "            -cog_range[0] / (cog_range[1] - cog_range[0]),\n",
    "            -heading_range[0] / (heading_range[1] - heading_range[0])\n",
    "        ])\n",
    "        self.scaler.scale_ = np.array([\n",
    "            1 / (lat_range[1] - lat_range[0]),\n",
    "            1 / (lon_range[1] - lon_range[0]),\n",
    "            1 / (sog_range[1] - sog_range[0]),\n",
    "            1 / (cog_range[1] - cog_range[0]),\n",
    "            1 / (heading_range[1] - heading_range[0])\n",
    "        ])\n",
    "        self.scaler.feature_names_in_ = np.array(['ìœ„ë„', 'ê²½ë„', 'SOG', 'COG', 'Heading'])\n",
    "\n",
    "\n",
    "    def load_and_preprocess(self):\n",
    "        input_seqs = []\n",
    "        output_seqs = []\n",
    "        count = 1\n",
    "        for file in os.listdir(self.data_dir):\n",
    "            if file.endswith('.csv'):\n",
    "                print(f\"---------- {count}ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\")\n",
    "                count += 1\n",
    "                df = pd.read_csv(os.path.join(self.data_dir, file), encoding='cp949')\n",
    "                df = self._preprocess_single_file(df)\n",
    "                in_seqs, out_seqs = self._extract_sequences(df)\n",
    "                input_seqs.extend(in_seqs)\n",
    "                output_seqs.extend(out_seqs)\n",
    "\n",
    "        return np.array(input_seqs), np.array(output_seqs)\n",
    "\n",
    "    def _preprocess_single_file(self, df):\n",
    "        df = df[['ì¼ì‹œ', 'ìœ„ë„', 'ê²½ë„', 'SOG', 'COG', 'Heading']].copy()\n",
    "        df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])\n",
    "        # ì¼ì‹œ ê¸°ì¤€ ë°ì´í„° sorting\n",
    "        df = df.sort_values('ì¼ì‹œ')\n",
    "        # NA ë°ì´í„° drop\n",
    "        df = df.dropna()\n",
    "        \n",
    "        # ê° ë°ì´í„°ë¥¼, ì¼ì‹œ ê¸°ì¤€ 1ë¶„ ë³„ ë³´ê°„ë²•ì„ ì ìš©í•´ transform ì ìš© ------------------------------------\n",
    "        # ì˜ˆ: 01:00:04, 01:00:06, 01:00:16 ë°ì´í„° -> í‰ê·  -> 01:00:00(1ì‹œ 0ë¶„)ìœ¼ë¡œ í•œ ìƒ˜í”Œ ìƒì„±\n",
    "        # ì—: 01:01:05, 01:01:12, 01:01:35, 01:01:44 ë°ì´í„° -> í‰ê·  -> 01:01:00(1ì‹œ 1ë¶„)ìœ¼ë¡œ í•œ ìƒ˜í”Œ ìƒì„±\n",
    "        # í‰ê· ì„ ì ìš©í•˜ëŠ” ë°ì´í„°ëŠ” ìœ„ë„, ê²½ë„, sog, cog ë°ì´í„° \n",
    "        df = df.set_index('ì¼ì‹œ').resample('1min').mean().interpolate()\n",
    "        df = df.reset_index()\n",
    "        # -----------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # ëª©ì ì§€ ì¢Œí‘œ: ë§ˆì§€ë§‰ ìœ„ì¹˜\n",
    "        dest_lat = df['ìœ„ë„'].iloc[-1]\n",
    "        dest_lon = df['ê²½ë„'].iloc[-1]\n",
    "        \n",
    "        # feature engineeringì„ ìœ„í•œ feature ì €ì¥ \n",
    "        df['dest_lat'] = dest_lat\n",
    "        df['dest_lon'] = dest_lon\n",
    "        return df\n",
    "\n",
    "    def _extract_sequences(self, df):\n",
    "        input_seqs = []\n",
    "        output_seqs = []\n",
    "    \n",
    "        total_len = self.input_seq_len + self.output_seq_len\n",
    "        for i in range(len(df) - total_len):\n",
    "            input_window = df.iloc[i:i+self.input_seq_len]\n",
    "            output_window = df.iloc[i+self.input_seq_len:i+total_len]\n",
    "    \n",
    "            # ì…ë ¥: ìœ„ë„, ê²½ë„, SOG, COG, Heading (ì •ê·œí™”)\n",
    "            input_scaled = self.scaler.transform(input_window[['ìœ„ë„', 'ê²½ë„', 'SOG', 'COG', 'Heading']])\n",
    "            input_seq = input_scaled\n",
    "            # ì¶œë ¥: ìœ„ë„, ê²½ë„, SOG, COG, Heading (ì •ê·œí™”)\n",
    "            output_seq = self.scaler.transform(output_window[['ìœ„ë„', 'ê²½ë„', 'SOG', 'COG', 'Heading']])\n",
    "    \n",
    "            input_seqs.append(input_seq)\n",
    "            output_seqs.append(output_seq)\n",
    "    \n",
    "        return input_seqs, output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "38ea158b-4b68-4eb3-8d76-4a81d9200ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- 1ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 2ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 3ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 4ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 5ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 6ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 7ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 8ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 9ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 10ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 11ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 12ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 13ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 14ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 15ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 16ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 17ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 18ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 19ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 20ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 21ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 22ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 23ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 24ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 25ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 26ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 27ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 28ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 29ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 30ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 31ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 32ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 33ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 34ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 35ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 36ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 37ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 38ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "---------- 39ë²ˆì§¸ íŒŒì¼ ì§„í–‰ ì¤‘ ----------\n",
      "Input sequences shape: (38701, 10, 5)\n",
      "Output sequences shape: (38701, 1, 5)\n",
      "\n",
      "Sample Input Sequence (ì²« ë²ˆì§¸ ìƒ˜í”Œ):\n",
      "[[0.86641333 0.29412708 0.185      0.72513889 0.725     ]\n",
      " [0.86622667 0.29345278 0.18466667 0.70814815 0.70925926]\n",
      " [0.86594524 0.29268006 0.18485714 0.70257937 0.70396825]\n",
      " [0.86570667 0.29212292 0.1845     0.69013889 0.69027778]\n",
      " [0.86517083 0.29128693 0.18275    0.66215278 0.66180556]\n",
      " [0.86447583 0.29051406 0.18375    0.65055556 0.65347222]\n",
      " [0.8638125  0.28977604 0.185      0.65256944 0.65416667]\n",
      " [0.86324222 0.28914646 0.18533333 0.65064815 0.65277778]\n",
      " [0.86261    0.28845618 0.184      0.65074074 0.65185185]\n",
      " [0.86201667 0.28780486 0.18333333 0.65       0.65277778]]\n",
      "\n",
      "Sample Output Sequence (ì²« ë²ˆì§¸ ìƒ˜í”Œ):\n",
      "[[0.86149556 0.28724438 0.18233333 0.64888889 0.65277778]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "# ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • (ì˜ˆ: routes í´ë” ì•ˆì— ì—¬ëŸ¬ ê°œì˜ csvê°€ ìˆëŠ” ê²½ìš°)\n",
    "data_dir = './routes'\n",
    "# ì „ì²˜ë¦¬ ê°ì²´ ìƒì„±\n",
    "preprocessor = AISPreprocessor(data_dir)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹¤í–‰\n",
    "input_seqs, output_seqs = preprocessor.load_and_preprocess()\n",
    "\n",
    "# ì¶œë ¥ í™•ì¸\n",
    "print(\"Input sequences shape:\", input_seqs.shape)   # (num_samples, 10, 5)\n",
    "print(\"Output sequences shape:\", output_seqs.shape) # (num_samples, 1, 5)\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„° ì¶œë ¥ (ì²« ìƒ˜í”Œ)\n",
    "print(\"\\nSample Input Sequence (ì²« ë²ˆì§¸ ìƒ˜í”Œ):\")\n",
    "print(input_seqs[5])\n",
    "\n",
    "print(\"\\nSample Output Sequence (ì²« ë²ˆì§¸ ìƒ˜í”Œ):\")\n",
    "print(output_seqs[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cadb13-0e76-4bb8-aff7-c70f3aa0a9c4",
   "metadata": {},
   "source": [
    "## ê²½ë¡œ ì˜ˆì¸¡ëª¨ë¸ \n",
    "\n",
    "- ì…ë ¥ ì‹œí€€ìŠ¤ (ìœ„ë„, ê²½ë„, SOG, COG, (ëª©ì ì§€-í˜„ì¬ìœ„ì¹˜ ìœ„ë„), (ëª©ì ì§€-í˜„ì¬ìœ„ì¹˜ ê²½ë„))\n",
    "  \n",
    "      â†“\n",
    "  \n",
    "- Linear í”„ë¡œì ì…˜ (input_size â†’ d_model)\n",
    "\n",
    "  \n",
    "      â†“\n",
    "\n",
    "  \n",
    "- Positional Encoding ì¶”ê°€\n",
    "\n",
    "  \n",
    "      â†“\n",
    "\n",
    "  \n",
    "- Transformer Encoder (Multi-head Self Attention, FeedForward)\n",
    "\n",
    "  \n",
    "      â†“\n",
    "\n",
    "  \n",
    "- Decoder (MLP)\n",
    "\n",
    "  \n",
    "      â†“\n",
    "\n",
    "  \n",
    "- ì¶œë ¥ (ì˜ˆ: ë‹¤ìŒ ì‹œì ì˜ ìœ„ë„, ê²½ë„, SOG, COG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c9a2a0bb-0c82-4bc0-a77c-dfaff70af749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4ef6a4d3-8a19-4788-97c2-42639aea30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9aa290fe-fab0-46ad-862f-0906c9a452bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# ìœ„ë„/ê²½ë„ ì˜¤ì°¨ ë¶€ë¶„ì— ì¤‘ì ì„ ë‘ê¸° ìœ„í•œ ì‚¬ìš©ì ì„¤ì • LOSS FUNCTION\n",
    "class CustomTrajectoryLoss(nn.Module):\n",
    "    def __init__(self, weight_location = 0.8):\n",
    "        super().__init__()\n",
    "        self.weight_location = weight_location\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        y_true = y_true.squeeze(1)\n",
    "        # y_pred, y_true: (batch_size, 5) â†’ [lat, lon, SOG, COG, Heading]\n",
    "        pred_lat, pred_lon = y_pred[:, 0], y_pred[:, 1]\n",
    "        true_lat, true_lon = y_true[:, 0], y_true[:, 1]\n",
    "        pred_sog, pred_cog = y_pred[:, 2], y_pred[:, 3]\n",
    "        true_sog, true_cog = y_true[:, 2], y_true[:, 3]\n",
    "        pred_heading = y_pred[:, 4]\n",
    "        true_heading = y_pred[:, 4]\n",
    "        \n",
    "        # 1. ìœ„ë„/ê²½ë„ loss\n",
    "        location_loss = 0.5*torch.square(pred_lat-true_lat) + 0.5*torch.square(pred_lon-true_lon)\n",
    "        location_loss = location_loss.mean()\n",
    "        # 2. sog, cog loss\n",
    "        else_loss = 0.5*torch.square(pred_sog-true_sog) + 0.5*torch.square(pred_cog-true_cog)\n",
    "        else_loss = else_loss.mean()\n",
    "        # 3. heading ì†ì‹¤ (360ë„ ê°ë„ ì°¨ì´ ê³ ë ¤)\n",
    "        heading_diff = torch.abs(pred_heading - true_heading)\n",
    "        heading_diff = torch.where(heading_diff > 180, 360 - heading_diff, heading_diff)\n",
    "        heading_loss = heading_diff.square().mean()\n",
    "        \n",
    "        # 4. í•©ì‚° Loss\n",
    "        total_loss =  total_loss = (\n",
    "            self.weight_location * location_loss +\n",
    "            0.5 * (1 - self.weight_location) * else_loss +\n",
    "            0.5 * (1 - self.weight_location) * heading_loss\n",
    "    )\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0da5701b-decf-46d5-b326-77f0a938e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "## ìœ„ì¹˜ ì •ë³´ ì „ë‹¬ì„ ìœ„í•œ ì •ì  í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even index\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd index\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "### ì‹œê³„ì—´ í•™ìŠµì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ íšŒê·€ ëª¨ë¸\n",
    "## d_model: ì£¼ëª©í•  inputì˜ íŠ¹ì§•ë“¤\n",
    "## nhead: ë©€í‹° í—¤ë“œ ì–´í…ì…˜ í—¤ë“œ ìˆ˜\n",
    "## dim_feedforward: FFN ì°¨ì› ìˆ˜\n",
    "# dim_feedforward = d_model * 4\n",
    "# d_model % n_head = 0\n",
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5, output_size=5, d_model=512, nhead=8, num_layers=4, dim_feedforward=1024, dropout=0.1):\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model) # í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ í†µí•´ ìˆœì„œ ì •ë³´ë¥¼ ì¶”ê°€\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, # inputì˜ íŠ¹ì§•ë“¤\n",
    "            nhead=nhead, # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ í—¤ë“œ ìˆ˜\n",
    "            dim_feedforward=dim_feedforward, # FFN ì°¨ì› ìˆ˜, ê¸°ë³¸ 2048\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\", # default=\"relu\"\n",
    "        )\n",
    "        # ì¸ì½”ë”\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # MLP ê¸°ë°˜ ë””ì½”ë”\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        x = self.input_proj(x)  # (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encoder(x)  # (batch_size, seq_len, d_model)\n",
    "        x = self.transformer_encoder(x)  # (batch_size, seq_len, d_model)\n",
    "        # ìê°€íšŒê·€ ì˜ˆì¸¡: ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©\n",
    "        x_last = x[:, -1, :]  # (batch_size, d_model)\n",
    "        out = self.decoder(x_last)  # (batch_size, output_size)\n",
    "\n",
    "        # ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ seq_len=1 ì¶•ì„ ë‹¤ì‹œ ì¶”ê°€\n",
    "        out = out.unsqueeze(1)  # (batch_size, 1, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "17497418-ed6e-49e4-b9d8-9ba78a7a36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_transformer_model(model, train_data, val_data=None, num_epochs=50, batch_size=128, learning_rate=1e-4, device='cpu'):\n",
    "    \"\"\"\n",
    "    model: TransformerPredictor ëª¨ë¸\n",
    "    train_data: (x_train_tensor, y_train_tensor)\n",
    "    val_data: (x_val_tensor, y_val_tensor)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    ## train_dataë¥¼ train / validation dataë¡œ ë¶„í•  --------------------------------\n",
    "    x_train, y_train = train_data\n",
    "    x_train_f, x_val, y_train_f, y_val = train_test_split(\n",
    "        x_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    \n",
    "    train_dataset = TensorDataset(x_train_f, y_train_f)\n",
    "    val_data = (x_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    \n",
    "    # Loss & Optimizer\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = CustomTrajectoryLoss(weight_location=0.95)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    \n",
    "    # ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "    writer = SummaryWriter(log_dir='./runs/transformer_experiment')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1) # gradient exploding ë°©ì§€ \n",
    "            outputs = model(batch_x)  # (batch, seq_len, output_size)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        # ë¡œê·¸íŒŒì¼ì— ê¸°ë¡\n",
    "        writer.add_scalar('Loss/Train', avg_loss, epoch)\n",
    "\n",
    "\n",
    "        # Validation ì§„í–‰\n",
    "        if val_data is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x_val, y_val = val_data\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                val_outputs = model(x_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                print(f\"           â†³ Val Loss: {val_loss.item():.6f}\")\n",
    "                # ë¡œê·¸íŒŒì¼ì— ê¸°ë¡\n",
    "                writer.add_scalar('Loss/Val', val_loss.item(), epoch)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dc8327fc-fd5b-4633-9fc5-69a12d9937ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Loss: 0.004445\n",
      "           â†³ Val Loss: 0.000184\n",
      "[Epoch 2/50] Train Loss: 0.000220\n",
      "           â†³ Val Loss: 0.000162\n",
      "[Epoch 3/50] Train Loss: 0.000134\n",
      "           â†³ Val Loss: 0.000206\n",
      "[Epoch 4/50] Train Loss: 0.000091\n",
      "           â†³ Val Loss: 0.000064\n",
      "[Epoch 5/50] Train Loss: 0.000073\n",
      "           â†³ Val Loss: 0.000118\n",
      "[Epoch 6/50] Train Loss: 0.000060\n",
      "           â†³ Val Loss: 0.000050\n",
      "[Epoch 7/50] Train Loss: 0.000058\n",
      "           â†³ Val Loss: 0.000056\n",
      "[Epoch 8/50] Train Loss: 0.000044\n",
      "           â†³ Val Loss: 0.000108\n",
      "[Epoch 9/50] Train Loss: 0.000045\n",
      "           â†³ Val Loss: 0.000060\n",
      "[Epoch 10/50] Train Loss: 0.000035\n",
      "           â†³ Val Loss: 0.000067\n",
      "[Epoch 11/50] Train Loss: 0.000035\n",
      "           â†³ Val Loss: 0.000048\n",
      "[Epoch 12/50] Train Loss: 0.000032\n",
      "           â†³ Val Loss: 0.000137\n",
      "[Epoch 13/50] Train Loss: 0.000031\n",
      "           â†³ Val Loss: 0.000052\n",
      "[Epoch 14/50] Train Loss: 0.000028\n",
      "           â†³ Val Loss: 0.000025\n",
      "[Epoch 15/50] Train Loss: 0.000035\n",
      "           â†³ Val Loss: 0.000035\n",
      "[Epoch 16/50] Train Loss: 0.000024\n",
      "           â†³ Val Loss: 0.000040\n",
      "[Epoch 17/50] Train Loss: 0.000023\n",
      "           â†³ Val Loss: 0.000024\n",
      "[Epoch 18/50] Train Loss: 0.000030\n",
      "           â†³ Val Loss: 0.000052\n",
      "[Epoch 19/50] Train Loss: 0.000024\n",
      "           â†³ Val Loss: 0.000053\n",
      "[Epoch 20/50] Train Loss: 0.000025\n",
      "           â†³ Val Loss: 0.000099\n",
      "[Epoch 21/50] Train Loss: 0.000022\n",
      "           â†³ Val Loss: 0.000027\n",
      "[Epoch 22/50] Train Loss: 0.000022\n",
      "           â†³ Val Loss: 0.000143\n",
      "[Epoch 23/50] Train Loss: 0.000019\n",
      "           â†³ Val Loss: 0.000081\n",
      "[Epoch 24/50] Train Loss: 0.000022\n",
      "           â†³ Val Loss: 0.000074\n",
      "[Epoch 25/50] Train Loss: 0.000021\n",
      "           â†³ Val Loss: 0.000025\n",
      "[Epoch 26/50] Train Loss: 0.000018\n",
      "           â†³ Val Loss: 0.000026\n",
      "[Epoch 27/50] Train Loss: 0.000019\n",
      "           â†³ Val Loss: 0.000016\n",
      "[Epoch 28/50] Train Loss: 0.000021\n",
      "           â†³ Val Loss: 0.000017\n",
      "[Epoch 29/50] Train Loss: 0.000015\n",
      "           â†³ Val Loss: 0.000039\n",
      "[Epoch 30/50] Train Loss: 0.000021\n",
      "           â†³ Val Loss: 0.000015\n",
      "[Epoch 31/50] Train Loss: 0.000016\n",
      "           â†³ Val Loss: 0.000024\n",
      "[Epoch 32/50] Train Loss: 0.000019\n",
      "           â†³ Val Loss: 0.000030\n",
      "[Epoch 33/50] Train Loss: 0.000015\n",
      "           â†³ Val Loss: 0.000020\n",
      "[Epoch 34/50] Train Loss: 0.000016\n",
      "           â†³ Val Loss: 0.000042\n",
      "[Epoch 35/50] Train Loss: 0.000016\n",
      "           â†³ Val Loss: 0.000027\n",
      "[Epoch 36/50] Train Loss: 0.000017\n",
      "           â†³ Val Loss: 0.000008\n",
      "[Epoch 37/50] Train Loss: 0.000020\n",
      "           â†³ Val Loss: 0.000020\n",
      "[Epoch 38/50] Train Loss: 0.000014\n",
      "           â†³ Val Loss: 0.000013\n",
      "[Epoch 39/50] Train Loss: 0.000011\n",
      "           â†³ Val Loss: 0.000080\n",
      "[Epoch 40/50] Train Loss: 0.000015\n",
      "           â†³ Val Loss: 0.000021\n",
      "[Epoch 41/50] Train Loss: 0.000015\n",
      "           â†³ Val Loss: 0.000023\n",
      "[Epoch 42/50] Train Loss: 0.000014\n",
      "           â†³ Val Loss: 0.000024\n",
      "[Epoch 43/50] Train Loss: 0.000013\n",
      "           â†³ Val Loss: 0.000007\n",
      "[Epoch 44/50] Train Loss: 0.000014\n",
      "           â†³ Val Loss: 0.000016\n",
      "[Epoch 45/50] Train Loss: 0.000012\n",
      "           â†³ Val Loss: 0.000021\n",
      "[Epoch 46/50] Train Loss: 0.000012\n",
      "           â†³ Val Loss: 0.000025\n",
      "[Epoch 47/50] Train Loss: 0.000013\n",
      "           â†³ Val Loss: 0.000009\n",
      "[Epoch 48/50] Train Loss: 0.000012\n",
      "           â†³ Val Loss: 0.000014\n",
      "[Epoch 49/50] Train Loss: 0.000012\n",
      "           â†³ Val Loss: 0.000029\n",
      "[Epoch 50/50] Train Loss: 0.000017\n",
      "           â†³ Val Loss: 0.000018\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ìƒì„±\n",
    "model = TransformerPredictor(input_size=5, output_size=5)\n",
    "\n",
    "# numpy â†’ torch tensorë¡œ ë³€í™˜\n",
    "input_tensor = torch.tensor(input_seqs, dtype=torch.float32)\n",
    "output_tensor = torch.tensor(output_seqs, dtype=torch.float32)\n",
    "# í•™ìŠµ\n",
    "train_transformer_model(model, (input_tensor, output_tensor), num_epochs=50, device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a996745-1886-480a-9290-3680d011abd9",
   "metadata": {},
   "source": [
    "## ìê°€íšŒê·€ ì˜ˆì¸¡ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "76b44b5b-29ac-42ec-a804-9ffacfa377b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model, \"route_predictor2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5ed67c4e-ec44-4b2b-ae1c-15dabe4f1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "## ìê°€íšŒê·€ë¥¼ ìœ„í•œ í•¨ìˆ˜\n",
    "## ì„ ë°•ì˜ ì˜ˆìƒ ê²½ë¡œê°€ ëª©ì ì§€ ì¸ê·¼ ë¶€ê·¼ì´ ë  ë•Œê¹Œì§€ ëª¨ë¸ì˜ ì˜ˆì¸¡ ë°˜ë³µ / ì„ê³„ê°’: max_steps\n",
    "def predict_autoregressive(model, initial_seq, dest_lat, dest_lon, scaler, max_steps, distance_threshold=0.1):\n",
    "    \"\"\"\n",
    "    model: í•™ìŠµëœ Transformer ëª¨ë¸\n",
    "    initial_seq: ì´ˆê¸° ì…ë ¥ ì‹œí€€ìŠ¤ (torch.Tensor), shape: (1, 10, 5), ì •ê·œí™”ëœ ê°’ì´ ë“¤ì–´ì™€ì•¼ ë¨.\n",
    "    dest_lat, dest_lon: ëª©ì ì§€ ì¢Œí‘œ\n",
    "    scaler: í•™ìŠµì— ì‚¬ìš©í•œ MinMaxScaler\n",
    "    device: 'cpu' or 'cuda'\n",
    "    max_steps: ìµœëŒ€ ì˜ˆì¸¡ ìŠ¤í… ìˆ˜\n",
    "    distance_threshold: ë„ì°©ì§€ì™€ ê±°ë¦¬ ì°¨ê°€ ì´ ê°’ ì´í•˜ì´ë©´ ë„ì°©ìœ¼ë¡œ ê°„ì£¼\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_seq = initial_seq.clone()  # input_seqì˜ ë³µì‚¬ ìƒì„±\n",
    "    all_preds = []\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        input_tensor = input_seq\n",
    "        # ì˜ˆì¸¡\n",
    "        with torch.no_grad():\n",
    "            pred = model(input_tensor).squeeze(0).cpu().numpy()  # (seq_len, output_size)\n",
    "\n",
    "        all_preds.append(pred)\n",
    "\n",
    "        ## ì˜ˆìƒ ê²½ë¡œ ë„ì°©ì§€ ë„ë‹¬ ì—¬ë¶€ í™•ì¸ ë§¤ì»¤ë‹ˆì¦˜ ----------------------------------------------------------\n",
    "        # ì˜ˆì¸¡í•œ ê²½ë¡œê°’ ì—­ì •ê·œí™”\n",
    "        pred_denorm = scaler.inverse_transform(pred.reshape(1, -1))[0][:5]\n",
    "        pred_lat, pred_lon = pred_denorm[:2]\n",
    "\n",
    "        # ë””ë²„ê¹… ì½”ë“œ\n",
    "        if(step % 10 == 0):\n",
    "            print(f\"[Step {step+1}] Predicted: ({pred_lat:.5f}, {pred_lon:.5f}) | \"\n",
    "                  f\"Target: ({dest_lat:.5f}, {dest_lon:.5f}) | \"\n",
    "                  f\"Î”Lat: {abs(pred_lat - dest_lat):.5f}, Î”Lon: {abs(pred_lon - dest_lon):.5f}\")\n",
    "    \n",
    "    \n",
    "        if abs(pred_lat - dest_lat) < distance_threshold and abs(pred_lon - dest_lon) < distance_threshold:\n",
    "            print(f\"ğŸš¢ ëª©ì ì§€ ë„ë‹¬ - Step: {step + 1}, {int(step/60)} ì‹œê°„ {step%60} ë¶„ ì†Œìš”\")\n",
    "            break\n",
    "\n",
    "        ## ------------------------------------------------------------------------------------------------\n",
    "            \n",
    "        ## ì…ë ¥ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸: ë‹¤ìŒ ì…ë ¥ì„ ë§Œë“¦ -----------------------------------------------\n",
    "        \n",
    "        # ì •ê·œí™”ëœ lat/lon/sog/cog/heading (5ê°œë§Œ ì‚¬ìš©)\n",
    "        pred_norm = pred.reshape(1, -1)[0]  # (5,)\n",
    "        next_input = pred_norm\n",
    "        # ---------------------------------------\n",
    "        # ê¸°ì¡´ ì‹œí€€ìŠ¤ì—ì„œ ê°€ì¥ ì• ë°ì´í„° ì œê±°, ìƒˆ ë°ì´í„° ì¶”ê°€\n",
    "        input_seq_np = input_seq.squeeze(0).numpy()\n",
    "        input_seq_np = np.vstack([input_seq_np[1:], next_input])\n",
    "        input_seq = torch.tensor([input_seq_np], dtype=torch.float32)  # (1, 10, 5)\n",
    "        # ----------------------------------------------------------------------\n",
    "    return np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b79886be-358d-451c-a36f-8efe56d1a0ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Predicted: (37.38929, 126.33118) | Target: (33.55000, 126.55000) | Î”Lat: 3.83929, Î”Lon: 0.21882\n",
      "[Step 11] Predicted: (37.10891, 126.16767) | Target: (33.55000, 126.55000) | Î”Lat: 3.55891, Î”Lon: 0.38233\n",
      "[Step 21] Predicted: (37.03115, 126.12748) | Target: (33.55000, 126.55000) | Î”Lat: 3.48115, Î”Lon: 0.42252\n",
      "[Step 31] Predicted: (36.98857, 126.10006) | Target: (33.55000, 126.55000) | Î”Lat: 3.43857, Î”Lon: 0.44994\n",
      "[Step 41] Predicted: (36.94251, 126.07428) | Target: (33.55000, 126.55000) | Î”Lat: 3.39251, Î”Lon: 0.47572\n",
      "[Step 51] Predicted: (36.88468, 126.04324) | Target: (33.55000, 126.55000) | Î”Lat: 3.33468, Î”Lon: 0.50676\n",
      "[Step 61] Predicted: (36.81230, 126.00491) | Target: (33.55000, 126.55000) | Î”Lat: 3.26230, Î”Lon: 0.54509\n",
      "[Step 71] Predicted: (36.72229, 125.95927) | Target: (33.55000, 126.55000) | Î”Lat: 3.17229, Î”Lon: 0.59073\n",
      "[Step 81] Predicted: (36.61604, 125.90851) | Target: (33.55000, 126.55000) | Î”Lat: 3.06604, Î”Lon: 0.64149\n",
      "[Step 91] Predicted: (36.51114, 125.86721) | Target: (33.55000, 126.55000) | Î”Lat: 2.96114, Î”Lon: 0.68279\n",
      "[Step 101] Predicted: (36.41785, 125.84447) | Target: (33.55000, 126.55000) | Î”Lat: 2.86785, Î”Lon: 0.70553\n",
      "[Step 111] Predicted: (36.33237, 125.82343) | Target: (33.55000, 126.55000) | Î”Lat: 2.78237, Î”Lon: 0.72657\n",
      "[Step 121] Predicted: (36.24747, 125.80299) | Target: (33.55000, 126.55000) | Î”Lat: 2.69747, Î”Lon: 0.74701\n",
      "[Step 131] Predicted: (36.15894, 125.78247) | Target: (33.55000, 126.55000) | Î”Lat: 2.60894, Î”Lon: 0.76753\n",
      "[Step 141] Predicted: (36.06381, 125.76132) | Target: (33.55000, 126.55000) | Î”Lat: 2.51381, Î”Lon: 0.78868\n",
      "[Step 151] Predicted: (35.95967, 125.73938) | Target: (33.55000, 126.55000) | Î”Lat: 2.40967, Î”Lon: 0.81062\n",
      "[Step 161] Predicted: (35.84358, 125.71506) | Target: (33.55000, 126.55000) | Î”Lat: 2.29358, Î”Lon: 0.83494\n",
      "[Step 171] Predicted: (35.70942, 125.68526) | Target: (33.55000, 126.55000) | Î”Lat: 2.15942, Î”Lon: 0.86474\n",
      "[Step 181] Predicted: (35.55180, 125.65520) | Target: (33.55000, 126.55000) | Î”Lat: 2.00180, Î”Lon: 0.89480\n",
      "[Step 191] Predicted: (35.36028, 125.62766) | Target: (33.55000, 126.55000) | Î”Lat: 1.81028, Î”Lon: 0.92234\n",
      "[Step 201] Predicted: (35.13581, 125.62155) | Target: (33.55000, 126.55000) | Î”Lat: 1.58581, Î”Lon: 0.92845\n",
      "[Step 211] Predicted: (34.93167, 125.66695) | Target: (33.55000, 126.55000) | Î”Lat: 1.38167, Î”Lon: 0.88305\n",
      "[Step 221] Predicted: (34.75071, 125.74370) | Target: (33.55000, 126.55000) | Î”Lat: 1.20071, Î”Lon: 0.80630\n",
      "[Step 231] Predicted: (34.59589, 125.81403) | Target: (33.55000, 126.55000) | Î”Lat: 1.04589, Î”Lon: 0.73597\n",
      "[Step 241] Predicted: (34.47976, 125.88510) | Target: (33.55000, 126.55000) | Î”Lat: 0.92976, Î”Lon: 0.66490\n",
      "[Step 251] Predicted: (34.39866, 125.98999) | Target: (33.55000, 126.55000) | Î”Lat: 0.84866, Î”Lon: 0.56001\n",
      "[Step 261] Predicted: (34.32166, 126.08709) | Target: (33.55000, 126.55000) | Î”Lat: 0.77166, Î”Lon: 0.46291\n",
      "[Step 271] Predicted: (34.24516, 126.16681) | Target: (33.55000, 126.55000) | Î”Lat: 0.69516, Î”Lon: 0.38319\n",
      "[Step 281] Predicted: (34.16692, 126.23883) | Target: (33.55000, 126.55000) | Î”Lat: 0.61692, Î”Lon: 0.31117\n",
      "[Step 291] Predicted: (34.09816, 126.30692) | Target: (33.55000, 126.55000) | Î”Lat: 0.54816, Î”Lon: 0.24308\n",
      "[Step 301] Predicted: (34.02254, 126.37859) | Target: (33.55000, 126.55000) | Î”Lat: 0.47254, Î”Lon: 0.17141\n",
      "[Step 311] Predicted: (33.92914, 126.44305) | Target: (33.55000, 126.55000) | Î”Lat: 0.37914, Î”Lon: 0.10695\n",
      "[Step 321] Predicted: (33.82921, 126.48616) | Target: (33.55000, 126.55000) | Î”Lat: 0.27921, Î”Lon: 0.06384\n",
      "[Step 331] Predicted: (33.74941, 126.51469) | Target: (33.55000, 126.55000) | Î”Lat: 0.19941, Î”Lon: 0.03531\n",
      "[Step 341] Predicted: (33.68937, 126.52621) | Target: (33.55000, 126.55000) | Î”Lat: 0.13937, Î”Lon: 0.02379\n",
      "ğŸš¢ ëª©ì ì§€ ë„ë‹¬ - Step: 349, 5 ì‹œê°„ 48 ë¶„ ì†Œìš”\n"
     ]
    }
   ],
   "source": [
    "from math import atan2, sqrt, degrees\n",
    "\n",
    "## ì´ˆê¸° 10ë¶„ ì‹œí€€ìŠ¤\n",
    "# ì˜ˆì‹œ: íŠ¹ì • CSV íŒŒì¼ì—ì„œ ì¸ì²œí•­ ì¶œë°œ ê²½ë¡œ í•˜ë‚˜ ë¡œë“œ\n",
    "pre = AISPreprocessor(data_dir='routes', input_seq_len=10, output_seq_len=1)\n",
    "\n",
    "df = pd.read_csv('routes/route_1_202002.csv', encoding='cp949', parse_dates=['ì¼ì‹œ'])\n",
    "df = pre._preprocess_single_file(df)\n",
    "initial_seq = df.iloc[:10]\n",
    "\n",
    "# MinMaxScaler ìˆ˜ë™ ì„¤ì • ----------------------------------------------------------\n",
    "# ì •ê·œí™” ë²”ìœ„ ì„¤ì •\n",
    "lat_range = (33.0, 38.0)\n",
    "lon_range = (124.0, 132.0)\n",
    "sog_range = (0.0, 100.0)\n",
    "cog_range = (0.0, 360.0)\n",
    "heading_range = (0.0, 360.0)\n",
    "\n",
    "input_scaler = MinMaxScaler()\n",
    "input_scaler.min_ = np.array([\n",
    "    -lat_range[0] / (lat_range[1] - lat_range[0]),\n",
    "    -lon_range[0] / (lon_range[1] - lon_range[0]),\n",
    "    -sog_range[0] / (sog_range[1] - sog_range[0]),\n",
    "    -cog_range[0] / (cog_range[1] - cog_range[0]),\n",
    "    -heading_range[0] / (heading_range[1] - heading_range[0])\n",
    "])\n",
    "input_scaler.scale_ = np.array([\n",
    "    1 / (lat_range[1] - lat_range[0]),\n",
    "    1 / (lon_range[1] - lon_range[0]),\n",
    "    1 / (sog_range[1] - sog_range[0]),\n",
    "    1 / (cog_range[1] - cog_range[0]),\n",
    "    1 / (heading_range[1] - heading_range[0])\n",
    "])\n",
    "input_scaler.feature_names_in_ = np.array(['ìœ„ë„', 'ê²½ë„', 'SOG', 'COG', 'Heading'])\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# ëª©ì ì§€ ì¢Œí‘œ ì„¤ì • - ì œì£¼í•­\n",
    "dest_lat = 33.55  \n",
    "dest_lon = 126.55  \n",
    "\n",
    "# ì…ë ¥ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "test_input_seq = []\n",
    "for _, row in initial_seq.iterrows():\n",
    "    # 1. ì •ê·œí™”ëœ ìœ„ë„, ê²½ë„, SOG, COG\n",
    "    scaled = input_scaler.transform([[row['ìœ„ë„'], row['ê²½ë„'], row['SOG'], row['COG'], row['Heading']]])[0]\n",
    "\n",
    "    # 4. ìµœì¢… ì…ë ¥ ë²¡í„° êµ¬ì„± (5ì°¨ì›)\n",
    "    input_row = list(scaled)\n",
    "    test_input_seq.append(input_row)\n",
    "\n",
    "test_input_seq = torch.tensor([test_input_seq], dtype=torch.float32)  # (1, 10, 5)\n",
    "\n",
    "# ëª©ì ì§€ ì¢Œí‘œ ì„¤ì •\n",
    "destination_lat = dest_lat\n",
    "destination_lon = dest_lon\n",
    "scaler = input_scaler\n",
    "\n",
    "# ì˜ˆì¸¡ ì‹¤í–‰\n",
    "preds = predict_autoregressive(\n",
    "    model=model,                      # í•™ìŠµëœ Transformer ëª¨ë¸\n",
    "    initial_seq=test_input_seq,       # ì´ˆê¸° ì…ë ¥ ì‹œí€€ìŠ¤ \n",
    "    dest_lat=destination_lat,         # ëª©ì ì§€ ìœ„ë„\n",
    "    dest_lon=destination_lon,         # ëª©ì ì§€ ê²½ë„\n",
    "    scaler=scaler,                    # í•™ìŠµì— ì‚¬ìš©ëœ MinMaxScaler                 \n",
    "    max_steps=5000                    # ì˜ˆì¸¡í•  ì‹œê°„ ê¸¸ì´ \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "347bd929-75ca-447b-945c-5a039768e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_preds(preds, scaler):\n",
    "    \"\"\"\n",
    "    ì—­ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ì—¬ ì›ë˜ì˜ ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "    preds: ì˜ˆì¸¡ëœ ê°’ë“¤ (numpy ë°°ì—´), shape: (steps, output_size)\n",
    "    scaler: í•™ìŠµì— ì‚¬ìš©ëœ MinMaxScaler\n",
    "    \"\"\"\n",
    "    # ìœ„ë„, ê²½ë„, SOG, COG, heading ê°’ë§Œ ì—­ì •ê·œí™”\n",
    "    preds_unscaled = preds.copy()  # ì˜ˆì¸¡ëœ ê°’ì„ ë³µì‚¬\n",
    "\n",
    "    # ìœ„ë„, ê²½ë„, SOG, COG, headingì„ ì—­ì •ê·œí™”\n",
    "    preds_unscaled[:, :5] = scaler.inverse_transform(preds_unscaled[:, :5])  # ì—­ì •ê·œí™”\n",
    "    return preds_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "61027563-5b24-4e9e-a6d7-58765434ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import AntPath\n",
    "\n",
    "def visualize_route(initial_seq, preds_inverse, dest_lat, dest_lon):\n",
    "    \"\"\"\n",
    "    ì˜ˆì¸¡ëœ ê²½ë¡œë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜\n",
    "    initial_seq: ì´ˆê¸° ì…ë ¥ ì‹œí€€ìŠ¤ (numpy ë°°ì—´), shape: (10, 6)\n",
    "    preds_inverse: ì—­ì •ê·œí™”ëœ ì˜ˆì¸¡ ê²°ê³¼, shape: (steps, 4)\n",
    "    dest_lat, dest_lon: ëª©ì ì§€ ì¢Œí‘œ\n",
    "    \"\"\"\n",
    "    # ì´ˆê¸° ìœ„ì¹˜\n",
    "    start = initial_seq[0, 0][:5]\n",
    "    start = scaler.inverse_transform([start])\n",
    "    start_lat = start[0][0] \n",
    "    start_lon = start[0][1]\n",
    "    # ì§€ë„ ìƒì„± (ì¶œë°œì§€ì™€ ëª©ì ì§€ê°€ ëª¨ë‘ ë³´ì´ë„ë¡ ì„¤ì •)\n",
    "    route_map = folium.Map(location=[start_lat, start_lon], zoom_start=6)\n",
    "\n",
    "    # ì‹œì‘ì , ëª©ì ì§€ ë§ˆì»¤ ì¶”ê°€\n",
    "    folium.Marker([start_lat, start_lon], tooltip='Start', icon=folium.Icon(color='green')).add_to(route_map)\n",
    "    folium.Marker([dest_lat, dest_lon], tooltip='Destination', icon=folium.Icon(color='red')).add_to(route_map)\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²½ë¡œ\n",
    "    route_coords = [[lat, lon] for lat, lon in preds_inverse[:, :2]]  # ìœ„ë„, ê²½ë„ë§Œ ì‚¬ìš©\n",
    "    # ì˜ˆì¸¡ ê²½ë¡œë¥¼ PolyLineìœ¼ë¡œ ì‹œê°í™”\n",
    "    folium.PolyLine(route_coords, color='blue', weight=3, tooltip=\"Predicted Route\").add_to(route_map)\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²½ë¡œì— ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ ì¶”ê°€\n",
    "    AntPath(route_coords).add_to(route_map)\n",
    "\n",
    "    return route_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "568a7ae6-4167-44c9-8fe5-2a2ffaf2c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì—­ì •ê·œí™” í›„ ì‹œê°í™”í•˜ëŠ” ì „ì²´ ì½”ë“œ\n",
    "def predict_and_visualize(model, initial_seq, dest_lat, dest_lon, scaler, max_steps=5000, distance_threshold=0.1):\n",
    "    preds = predict_autoregressive(model, initial_seq, dest_lat, dest_lon, scaler, max_steps, distance_threshold)\n",
    "    # ì—­ì •ê·œí™”\n",
    "    preds = preds.squeeze(1)\n",
    "    preds_inverse = inverse_transform_preds(preds, scaler)\n",
    "    # ì˜ˆì¸¡ ê²½ë¡œ ì‹œê°í™”\n",
    "    route_map = visualize_route(initial_seq.numpy(), preds_inverse, dest_lat, dest_lon)\n",
    "\n",
    "    return route_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "79138a8c-1d98-4aa2-ad14-b8062fe8f02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Predicted: (37.38929, 126.33118) | Target: (33.55000, 126.55000) | Î”Lat: 3.83929, Î”Lon: 0.21882\n",
      "[Step 11] Predicted: (37.10891, 126.16767) | Target: (33.55000, 126.55000) | Î”Lat: 3.55891, Î”Lon: 0.38233\n",
      "[Step 21] Predicted: (37.03115, 126.12748) | Target: (33.55000, 126.55000) | Î”Lat: 3.48115, Î”Lon: 0.42252\n",
      "[Step 31] Predicted: (36.98857, 126.10006) | Target: (33.55000, 126.55000) | Î”Lat: 3.43857, Î”Lon: 0.44994\n",
      "[Step 41] Predicted: (36.94251, 126.07428) | Target: (33.55000, 126.55000) | Î”Lat: 3.39251, Î”Lon: 0.47572\n",
      "[Step 51] Predicted: (36.88468, 126.04324) | Target: (33.55000, 126.55000) | Î”Lat: 3.33468, Î”Lon: 0.50676\n",
      "[Step 61] Predicted: (36.81230, 126.00491) | Target: (33.55000, 126.55000) | Î”Lat: 3.26230, Î”Lon: 0.54509\n",
      "[Step 71] Predicted: (36.72229, 125.95927) | Target: (33.55000, 126.55000) | Î”Lat: 3.17229, Î”Lon: 0.59073\n",
      "[Step 81] Predicted: (36.61604, 125.90851) | Target: (33.55000, 126.55000) | Î”Lat: 3.06604, Î”Lon: 0.64149\n",
      "[Step 91] Predicted: (36.51114, 125.86721) | Target: (33.55000, 126.55000) | Î”Lat: 2.96114, Î”Lon: 0.68279\n",
      "[Step 101] Predicted: (36.41785, 125.84447) | Target: (33.55000, 126.55000) | Î”Lat: 2.86785, Î”Lon: 0.70553\n",
      "[Step 111] Predicted: (36.33237, 125.82343) | Target: (33.55000, 126.55000) | Î”Lat: 2.78237, Î”Lon: 0.72657\n",
      "[Step 121] Predicted: (36.24747, 125.80299) | Target: (33.55000, 126.55000) | Î”Lat: 2.69747, Î”Lon: 0.74701\n",
      "[Step 131] Predicted: (36.15894, 125.78247) | Target: (33.55000, 126.55000) | Î”Lat: 2.60894, Î”Lon: 0.76753\n",
      "[Step 141] Predicted: (36.06381, 125.76132) | Target: (33.55000, 126.55000) | Î”Lat: 2.51381, Î”Lon: 0.78868\n",
      "[Step 151] Predicted: (35.95967, 125.73938) | Target: (33.55000, 126.55000) | Î”Lat: 2.40967, Î”Lon: 0.81062\n",
      "[Step 161] Predicted: (35.84358, 125.71506) | Target: (33.55000, 126.55000) | Î”Lat: 2.29358, Î”Lon: 0.83494\n",
      "[Step 171] Predicted: (35.70942, 125.68526) | Target: (33.55000, 126.55000) | Î”Lat: 2.15942, Î”Lon: 0.86474\n",
      "[Step 181] Predicted: (35.55180, 125.65520) | Target: (33.55000, 126.55000) | Î”Lat: 2.00180, Î”Lon: 0.89480\n",
      "[Step 191] Predicted: (35.36028, 125.62766) | Target: (33.55000, 126.55000) | Î”Lat: 1.81028, Î”Lon: 0.92234\n",
      "[Step 201] Predicted: (35.13581, 125.62155) | Target: (33.55000, 126.55000) | Î”Lat: 1.58581, Î”Lon: 0.92845\n",
      "[Step 211] Predicted: (34.93167, 125.66695) | Target: (33.55000, 126.55000) | Î”Lat: 1.38167, Î”Lon: 0.88305\n",
      "[Step 221] Predicted: (34.75071, 125.74370) | Target: (33.55000, 126.55000) | Î”Lat: 1.20071, Î”Lon: 0.80630\n",
      "[Step 231] Predicted: (34.59589, 125.81403) | Target: (33.55000, 126.55000) | Î”Lat: 1.04589, Î”Lon: 0.73597\n",
      "[Step 241] Predicted: (34.47976, 125.88510) | Target: (33.55000, 126.55000) | Î”Lat: 0.92976, Î”Lon: 0.66490\n",
      "[Step 251] Predicted: (34.39866, 125.98999) | Target: (33.55000, 126.55000) | Î”Lat: 0.84866, Î”Lon: 0.56001\n",
      "[Step 261] Predicted: (34.32166, 126.08709) | Target: (33.55000, 126.55000) | Î”Lat: 0.77166, Î”Lon: 0.46291\n",
      "[Step 271] Predicted: (34.24516, 126.16681) | Target: (33.55000, 126.55000) | Î”Lat: 0.69516, Î”Lon: 0.38319\n",
      "[Step 281] Predicted: (34.16692, 126.23883) | Target: (33.55000, 126.55000) | Î”Lat: 0.61692, Î”Lon: 0.31117\n",
      "[Step 291] Predicted: (34.09816, 126.30692) | Target: (33.55000, 126.55000) | Î”Lat: 0.54816, Î”Lon: 0.24308\n",
      "[Step 301] Predicted: (34.02254, 126.37859) | Target: (33.55000, 126.55000) | Î”Lat: 0.47254, Î”Lon: 0.17141\n",
      "[Step 311] Predicted: (33.92914, 126.44305) | Target: (33.55000, 126.55000) | Î”Lat: 0.37914, Î”Lon: 0.10695\n",
      "[Step 321] Predicted: (33.82921, 126.48616) | Target: (33.55000, 126.55000) | Î”Lat: 0.27921, Î”Lon: 0.06384\n",
      "[Step 331] Predicted: (33.74941, 126.51469) | Target: (33.55000, 126.55000) | Î”Lat: 0.19941, Î”Lon: 0.03531\n",
      "[Step 341] Predicted: (33.68937, 126.52621) | Target: (33.55000, 126.55000) | Î”Lat: 0.13937, Î”Lon: 0.02379\n",
      "ğŸš¢ ëª©ì ì§€ ë„ë‹¬ - Step: 349, 5 ì‹œê°„ 48 ë¶„ ì†Œìš”\n"
     ]
    }
   ],
   "source": [
    "# ì˜ˆì‹œ: ì´ˆê¸° ì‹œí€€ìŠ¤ì™€ ëª©ì ì§€ ì¢Œí‘œë¡œ ì˜ˆì¸¡ ë° ì‹œê°í™”\n",
    "route_map = predict_and_visualize(model, test_input_seq, dest_lat=33.55, dest_lon=126.55, scaler=scaler)\n",
    "route_map.save('predicted_route_map_v3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894cf2cb-0bda-4f8e-9a2c-c7f079f1f1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
