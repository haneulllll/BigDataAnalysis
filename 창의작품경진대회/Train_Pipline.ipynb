{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d719b8e-3373-451b-a73c-a95aeebe2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf7a08-a1d4-4e50-a243-f4f1d227e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "class DestinationLoss(nn.Module):\n",
    "    def __init__(self, weight_main=0.99, cog_weight=0.9):\n",
    "        super().__init__()\n",
    "        self.weight_main = weight_main\n",
    "        self.cog_weight = cog_weight\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # [[]] 꼴로 변환\n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        y_true = y_true.squeeze(1)\n",
    "        \n",
    "        # 예측 결과 분해\n",
    "        pred_lat, pred_lon = y_pred[:, 0], y_pred[:, 1]\n",
    "        true_lat, true_lon = y_true[:, 0], y_true[:, 1]\n",
    "        pred_sog, pred_cog = y_pred[:, 2], y_pred[:, 3]\n",
    "        true_sog, true_cog = y_true[:, 2], y_true[:, 3]\n",
    "\n",
    "        # 위치 오차 (위경도 기준)\n",
    "        loc_loss = F.mse_loss(pred_lat, true_lat) + F.mse_loss(pred_lon, true_lon)\n",
    "\n",
    "        # SOG, COG 오차\n",
    "        motion_loss = (\n",
    "            (1-self.cog_weight) * F.mse_loss(pred_sog, true_sog) + \n",
    "            self.cog_weight * F.mse_loss(pred_cog, true_cog)\n",
    "        )\n",
    "\n",
    "        # 최종 Loss: 위치 오차 + SOG, COG 오차\n",
    "        total_loss = (\n",
    "            self.weight_main * loc_loss +\n",
    "            (1 - self.weight_main) * motion_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece371e-2587-4ed3-a18f-41bf997654c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Validation Pipline\n",
    "def train_transformer_model(model, train_data, val_data=None, num_epochs=20, batch_size=128, learning_rate=1e-5, device='cpu'):\n",
    "    model.to(device)\n",
    "    # VAL LOSS 가장 낮은 모델 저장 경로\n",
    "    save_path='model_history/best_model.pth'\n",
    "    best_val_loss = 1\n",
    "    \n",
    "    ## train_data를 train / validation data로 분할 --------------------------------\n",
    "    x_train, y_train = train_data\n",
    "    x_train_f, x_val, y_train_f, y_val = train_test_split(\n",
    "        x_train, y_train, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = TensorDataset(x_train_f, y_train_f)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    val_dataset = TensorDataset(x_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    \n",
    "    # Loss & Optimizer -----------------------------------------------------------\n",
    "    criterion = DestinationLoss(weight_main=0.999)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    # ----------------------------------------------------------------------------\n",
    "    \n",
    "    # Learning-rate annealing\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # ✅ Noise Injection ---------------------------------------------------------\n",
    "            # Feature-wise noise 적용\n",
    "            feature_noise_std = torch.tensor([1e-5, 1e-5, 1e-3, 1e-2, 1e-2], device=batch_x.device)\n",
    "            noise = torch.randn_like(batch_x) * feature_noise_std\n",
    "            batch_x_noisy = batch_x + noise\n",
    "            # ----------------------------------------------------------------------------    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x_noisy)  \n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            # gradient exploding 방지용 Cliping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1) \n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_x, val_y in val_loader:\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                val_outputs = model(val_x)\n",
    "                val_loss = criterion(val_outputs, val_y)\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            print(f\"           ↳ Val Loss: {avg_val_loss:.6f}\")\n",
    "            \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model, save_path)\n",
    "            print(f\"           ↳ ✅ Best model saved (Val Loss: {best_val_loss:.6f})\")\n",
    "            \n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
