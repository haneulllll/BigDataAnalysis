{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1775e6-9f4e-4415-8ae1-1faf705edfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트 \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43a470-3a41-4f1b-ae4d-8aac8e0a9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트랜스포머 모델\n",
    "# 1.위치 정보 전달을 위한 정적 포지셔널 인코딩\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even index\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd index\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2.시계열 학습을 위한 트랜스포머 회귀 모델\n",
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5, output_size=4, d_model=256, nhead=16, num_layers=3, dim_feedforward=1024, dropout=0.2, use_attention_pool=True):\n",
    "        super(TransformerPredictor, self).__init__()\n",
    "        # Attention pooling ------------------------------------------------------------------\n",
    "        self.use_attention_pool = use_attention_pool\n",
    "        if self.use_attention_pool:\n",
    "            self.attn_pool = nn.Sequential(\n",
    "                nn.Linear(d_model, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 1)  # 각 time step에 대한 score 출력\n",
    "            )\n",
    "        # --------------------------------------------------------------------------------------\n",
    "        # Input Projection ---------------------------------------------------------------------\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, d_model)\n",
    "        )\n",
    "        # --------------------------------------------------------------------------------------\n",
    "        # Encoder ------------------------------------------------------------------------------\n",
    "        self.pos_encoder = PositionalEncoding(d_model) # 포지셔널 인코딩을 통해 순서 정보를 추가\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, # input의 특징들\n",
    "            nhead=nhead, # 멀티 헤드 어텐션 헤드 수\n",
    "            dim_feedforward=dim_feedforward, # FFN 차원 수, 기본 2048\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\", # default=\"relu\"\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        # --------------------------------------------------------------------------------------\n",
    "        # Decoder ------------------------------------------------------------------------------\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            \n",
    "            nn.Linear(256, output_size)\n",
    "        )\n",
    "        # --------------------------------------------------------------------------------------\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)  \n",
    "        x = self.pos_encoder(x)  \n",
    "        x = self.transformer_encoder(x)  \n",
    "        \n",
    "        if self.use_attention_pool:\n",
    "            # Attention score 계산\n",
    "            attn_weights = self.attn_pool(x)  \n",
    "            attn_weights = torch.softmax(attn_weights, dim=1)  \n",
    "            x_last = (attn_weights * x).sum(dim=1) \n",
    "        else:\n",
    "            # Mean Pooling\n",
    "            x_last = x[:, -2:, :].mean(dim=1)\n",
    "            \n",
    "        out = self.decoder(x_last)  \n",
    "        out = out.unsqueeze(1)  \n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
